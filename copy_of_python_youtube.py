# -*- coding: utf-8 -*-
"""Copy of python_youtube.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1liFDHZ7BswHni9OH-4FYI-HeQXTJmE_M
"""

!pip install --upgrade youtube-dl
!pip install git+https://github.com/Cupcakus/pafy
import pandas as pd
import pafy
import re
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk import pos_tag
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk.download('wordnet')
from nltk.corpus import wordnet
nltk.download('averaged_perceptron_tagger')
pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
from textblob import TextBlob
from apiclient.discovery import build

!pip3 install pyrebase
import pyrebase
config = {
  "apiKey": "AIzaSyBheSQBRHe4lvDiXBC65O1KBC0oy_JWSjA",
  "authDomain": "sentiments-6a0bd.firebaseapp.com",
  "databaseURL": "https://sentiments-6a0bd-default-rtdb.firebaseio.com",
  "projectId": "sentiments-6a0bd",
  "storageBucket": "sentiments-6a0bd.appspot.com",
  "messagingSenderId" : "457865337160",
  "appId": "1:457865337160:web:3de636099ed86554537b7d",
  "measurementId": "G-4FQMQZCBP4"
};
firebase = pyrebase.initialize_app(config)
storage = firebase.storage()
database = firebase.database()
data1={}

def clean(text):
    text = re.sub('[^A-Za-z]+', ' ', text)
    return text
def token_stop_pos(text):
    tags = pos_tag(word_tokenize(text))
    newlist = []
    for word, tag in tags:  
        if word.lower() not in set(stopwords.words('english')):
           newlist.append(tuple([word, pos_dict.get(tag[0])]))
    return newlist
def lemmatize(pos_data):
    lemma_rew = " "
    for word, pos in pos_data:
      if not pos:
        lemma = word
        lemma_rew = lemma_rew + " " + lemma
      else:
        lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)
        lemma_rew = lemma_rew + " " + lemma
    return lemma_rew

def getSubjectivity(review):
    return TextBlob(review).sentiment.subjectivity
def getPolarity(review):
        return TextBlob(review).sentiment.polarity

def analysis(score):
    if score < 0:
        return 'Negative'
    elif score == 0:
        return 'Neutral'
    else:
        return 'Positive'
def function_analyzer(url):
  video = pafy.new(url)
  data=[]
  video_id=video.videoid
  data1["video_id"]=video_id
  youtube = build('youtube','v3',developerKey="AIzaSyCwICpI1QaWf_EF3Rrc7X3BTIa1x5BOgS4")
  video_response=youtube.commentThreads().list(part='snippet,replies',videoId=video_id).execute()
  replies=[]
  l1=[]
  i=0
  while i<1000: 
        i+=1
        for item in video_response['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            l1.append(comment)
            replies = []
        if 'nextPageToken' in video_response:
            video_response = youtube.commentThreads().list(part = 'snippet,replies',videoId = video_id).execute()
        else:
            break
  df=pd.DataFrame(data={"comments" :l1})
  df['Cleaned Reviews'] = df['comments'].apply(clean)

  pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}
  df['POS tagged'] = df['Cleaned Reviews'].apply(token_stop_pos)
  df['Lemma'] = df['POS tagged'].apply(lemmatize)
  #print(df.head())
  fin_data = pd.DataFrame(df[['comments', 'Lemma']])
  fin_data['Polarity'] = fin_data['Lemma'].apply(getPolarity) 
  fin_data['Analysis'] = fin_data['Polarity'].apply(analysis)
  tb_counts = fin_data.Analysis.value_counts()
  sentiment=fin_data['Analysis'].value_counts().idxmax()
  x=fin_data[fin_data['Analysis']=='Neutral']
  data.append(str(video.title))
  data.append(str(video.author))
  data.append(str(video.duration))
  data.append(str(video.likes))
  data.append(str(video.dislikes))
  data.append(str(video.viewcount))
  data.append(str(video.rating))
  data.append(str(sentiment))
  data.append(str((tb_counts[0]/float(fin_data.shape[0]))*100))   #sentiment percentage
  lst=x['comments'].iloc[0:5]
  data.append(lst)    #top comments

  return data

lst=function_analyzer("https://www.youtube.com/watch?v=YbJOTdZBX1g")
colname =  ["Title","Author","Duration","Likes","Dislikes","ViewCount","Rating","Sentiment_of_Comments","Top_Comments %"]
i=0

for item in lst:
  if(i<len(colname)):
    print(colname[i],item,sep=" : ")
    data1[colname[i]]=item
  else:
    print(item)
  i+=1
database.child("Video Sentiments")
#data = {"key1": a}
database.set(data1)





